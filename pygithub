#!/usr/bin/python
# -*- coding:utf-8 -*-
#python3，抓github内容，登录下

import requests
import pymysql.cursors
import xlwt
from bs4 import BeautifulSoup

def get_effect_data(data):
    results = list()
    soup = BeautifulSoup(data, 'html.parser')
    projects = soup.find_all('div', class_='user-list-item f5 py-4 d-flex')
    for project in projects:
        #中文名
        if project.find('div', attrs={'class': 'd-block d-md-inline f4 mt-2 mt-md-0 ml-md-1'}):
            project_language = project.find('div', attrs={'class': 'd-block d-md-inline f4 mt-2 mt-md-0 ml-md-1'}).get_text().strip()
        else:
            project_language = ''
        #地区
        if project.find('li', attrs={'class': 'mt-1 mt-md-0 mr-md-3'}):
            project_starts = project.find('li', attrs={'class': 'mt-1 mt-md-0 mr-md-3'}).get_text().strip()
        else:
            project_starts = ''
        #邮箱
        if  project.find('a', attrs={'class': 'muted-link'}):
            update_desc = project.find('a', attrs={'class': 'muted-link'}).get_text().strip()
        else:
            update_desc = ''
        result = (project_language, project_starts, update_desc)
        results.append(result)
    return results


def get_response_data(page,q,cookie):

    request_url = 'https://github.com/search'
    params = { 'q': q , 'type': 'users', 'p': page}

    resp = requests.get(request_url,params,cookies=cookie)
    return resp.text


def insert_datas(data):
    connection = pymysql.connect(host='localhost',
                                 user='root',
                                 password='123456',
                                 db='test',
                                 charset='utf8mb4',
                                 cursorclass=pymysql.cursors.DictCursor)
    try:
        with connection.cursor() as cursor:
            sql = 'insert into project_info(project_writer, project_name, project_language, project_starts, update_desc) VALUES (%s, %s, %s, %s, %s)'
            cursor.executemany(sql, data)
            connection.commit()
    except:
        connection.close()

#github登录
def uers_login():
    r1 = requests.get('https://github.com/login')
    soup = BeautifulSoup(r1.text, 'html.parser')
    s1 = soup.find(name='input', attrs={'name': 'authenticity_token'}).get('value')
    r1_cookies = r1.cookies.get_dict()
    r2 = requests.post('https://github.com/session',data={
        'commit': 'Sign in',
        'utf8': '✓',
        'authenticity_token': s1,
        'login': '799718014@qq.com',
        'password': 'lzxqq2316197'
    },cookies=r1_cookies)
    return r2.cookies.get_dict()

#调用execl函数
def rw_execl(data,fielname):
    # 写入execl
    wbk = xlwt.Workbook()
    sheet = wbk.add_sheet('sheet 1')
    # indexing is zero based, row then column
    sheet.write(0, 0, '昵称')
    sheet.write(0, 1, '地区')
    sheet.write(0, 2, '邮箱')
    i = 1
    for news in data:
        title = news[0]
        news_url = news[1]
        label = news[2]
        sheet.write(i, 0, title)
        sheet.write(i, 1, news_url)
        sheet.write(i, 2, label)
        i += 1
    wbk.save(fielname + '.xls')  # 默认保存在桌面上
    print("数据插入成功")


if __name__ == '__main__':
    word = input("请输入软件语言: ")
    print("您的软件语言关键词是："+word+"，接下来将下载100页的数据，过程将非常慢，请耐心等候")
    total_page = 100 # 爬虫数据的总页数
    datas = list()
    cookie = uers_login()
    for page in range(total_page):
        res_data = get_response_data(page + 1,'language:'+word+' location:china',cookie)
        data = get_effect_data(res_data)
        datas += data

    #insert_datas(datas)    //写到数据库
    #写到execl中
    rw_execl(datas,"test")
